{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPm3XbhHR4Nvs41cIz5MR7H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAPEVszvMWjt","executionInfo":{"status":"ok","timestamp":1694384652005,"user_tz":-120,"elapsed":19049,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"84682cff-b30b-4465-b328-cee807922ed4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertModel\n","\n","class ExpressiveParaphraser(nn.Module):\n","    def __init__(self, model_name):\n","        super(ExpressiveParaphraser, self).__init__()\n","        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n","        self.bert = BertModel.from_pretrained(model_name)\n","        self.signal_embedding = nn.Embedding(2, self.bert.config.hidden_size)  # Match the hidden size\n","        self.fc = nn.Linear(self.bert.config.hidden_size, self.tokenizer.vocab_size)\n","\n","    def forward(self, input_ids, attention_mask, signal, token_type_ids=None):\n","        signal_emb = self.signal_embedding(signal).unsqueeze(1).repeat(1, input_ids.size(1), 1)  # Repeat for the sequence length\n","        input_emb = self.bert.embeddings(input_ids)\n","\n","        # Combining the signal embedding with input embeddings\n","        combined_emb = input_emb + signal_emb\n","\n","        # Passing combined embeddings through BERT\n","        outputs = self.bert(inputs_embeds=combined_emb, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state\n","        logits = self.fc(sequence_output)\n","\n","        return logits\n","\n","\n","# If you have a GPU available on Colab\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = \"bert-base-uncased\"\n","model = ExpressiveParaphraser(model_name).to(device)\n"],"metadata":{"id":"4K5_d00QMhzk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = model.tokenizer([\"I love apples.\"], return_tensors=\"pt\", padding=True, truncation=True)\n","signal = torch.tensor([1]).to(device)  # for expressive paraphrase\n","\n","inputs = {key: val.to(device) for key, val in inputs.items()}\n","\n","with torch.no_grad():\n","    logits = model(**inputs, signal=signal)\n","\n","predicted_token_ids = torch.argmax(logits, dim=-1)\n","predicted_text = model.tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n","print(predicted_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iaf3CkJ5M5RP","executionInfo":{"status":"ok","timestamp":1694385304195,"user_tz":-120,"elapsed":722,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"0bb48590-1d9e-4498-899b-ba35e3e18183"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hundreds camps windows hundreds degraded north\n"]}]}]}