{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOG7n6GWtItSA4gOtD/1OHV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_68ybw-U-GUJ","executionInfo":{"status":"ok","timestamp":1708362056188,"user_tz":-60,"elapsed":2698,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"08cdf3b5-9e36-4447-cff0-e20978b74659"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["ds = 'manual' # 'mrpc','manual'\n","\n","input_path = '/content/drive/My Drive/Colab Notebooks/5_Corpora/corpora/'\n","pretrained_path = '/content/drive/My Drive/Colab Notebooks/8_Text_Paraphrasing/pretrained/t5-control-codes'\n","file_path = f'{ds}-triplet-corpus.csv'\n","print(input_path + file_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SsPT__qf-DhS","executionInfo":{"status":"ok","timestamp":1708362056188,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"a098d4f9-6e03-4141-d5f7-fcadfe2bdbf3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/5_Corpora/corpora/manual-triplet-corpus.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv(input_path + file_path)\n","train_df, val_df = train_test_split(df, test_size=0.1)\n","print(train_df.shape, val_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufudWd7ECSQJ","executionInfo":{"status":"ok","timestamp":1708362057302,"user_tz":-60,"elapsed":1126,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"c730c6f7-f566-4413-f1d4-bb989e923261"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(190, 38) (22, 38)\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","# Data Processing: Preparing the dataset with control codes for sentiment\n","class ParaphraseTripletDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length=512):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","        # Initialize lists to hold processed inputs and targets\n","        self.input_texts = []\n","        self.target_texts = []\n","\n","        # Process the dataframe\n","        for _, row in dataframe.iterrows():\n","            # Positive paraphrase\n","            pos_input_text = f\"paraphrase: [POS] {row['original']}\"\n","            pos_target_text = row['positive']\n","            self.input_texts.append(pos_input_text)\n","            self.target_texts.append(pos_target_text)\n","\n","            # Negative paraphrase\n","            neg_input_text = f\"paraphrase: [NEG] {row['original']}\"\n","            neg_target_text = row['negative']\n","            self.input_texts.append(neg_input_text)\n","            self.target_texts.append(neg_target_text)\n","\n","    def __len__(self):\n","        return len(self.input_texts)\n","\n","    def __getitem__(self, idx):\n","        input_text = self.input_texts[idx]\n","        target_text = self.target_texts[idx]\n","\n","        # Tokenize input and target texts\n","        # input_encodings = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n","        # target_encodings = self.tokenizer(target_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n","\n","        input_encodings = self.tokenizer.encode_plus(input_text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","        target_encodings = self.tokenizer.encode_plus(target_text, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","\n","        # input_ids = input_encodings['input_ids'].squeeze()  # Remove batch dimension\n","        # attention_mask = input_encodings['attention_mask'].squeeze()  # Remove batch dimension\n","        # target_ids = target_encodings['input_ids'].squeeze()  # Remove batch dimension\n","\n","        input_ids = input_encodings['input_ids'].flatten()  # Remove batch dimension\n","        attention_mask = input_encodings['attention_mask'].flatten()  # Remove batch dimension\n","        target_ids = target_encodings['input_ids'].flatten()  # Remove batch dimension\n","\n","\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": target_ids\n","        }\n"],"metadata":{"id":"-oYIbqltBcHc","executionInfo":{"status":"ok","timestamp":1708362059612,"user_tz":-60,"elapsed":2311,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","from torch.optim import AdamW\n","import torch\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from tqdm.auto import tqdm\n","\n","tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","model = T5ForConditionalGeneration.from_pretrained('t5-small')\n","\n","# Assuming device setup for CUDA if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avY4vE5UBR_5","executionInfo":{"status":"ok","timestamp":1708362065308,"user_tz":-60,"elapsed":5700,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"b6323293-2445-411d-ce84-03d0d8c28e39"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 8)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-5): 5 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 8)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-5): 5 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["def train_model(dataset, model, tokenizer, device, epochs=3, batch_size=8, learning_rate=5e-5):\n","    model.train()\n","    optimizer = AdamW(model.parameters(), lr=learning_rate)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for batch in tqdm(dataloader):\n","            optimizer.zero_grad()\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            # labels[labels == tokenizer.pad_token_id] = -100  # Ignore pad tokens in labels\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(dataloader)\n","        print(f\"Epoch: {epoch+1}, Loss: {avg_loss:.2f}\")"],"metadata":{"id":"V9ppGJKSBm2a","executionInfo":{"status":"ok","timestamp":1708362065308,"user_tz":-60,"elapsed":31,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def eval_model(dataset, model, tokenizer, device, batch_size=8):\n","    model.eval()\n","    dataloader = DataLoader(dataset, batch_size=batch_size)\n","\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","\n","            total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    print(f\"Validation Loss: {avg_loss:.2f}\")"],"metadata":{"id":"kKRgIp_3BsjK","executionInfo":{"status":"ok","timestamp":1708362065309,"user_tz":-60,"elapsed":32,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def generate_paraphrase(input_text, sentiment, model, tokenizer, device):\n","    model.eval()\n","    input_text = f\"paraphrase: [{sentiment.upper()}] {input_text}\"\n","    # print(input_text)\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n","    # print(input_ids)\n","    attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n","    # print(attention_mask)\n","\n","    # with torch.no_grad():\n","        # outputs = model.generate(input_ids=input_ids, max_length=512, num_beams=5, early_stopping=True)\n","    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=512)\n","    # print(outputs)\n","\n","    paraphrase = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return paraphrase\n"],"metadata":{"id":"WIoTXA5IBvu4","executionInfo":{"status":"ok","timestamp":1708362065309,"user_tz":-60,"elapsed":31,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# def generate_paraphrase(input_sentence, model, tokenizer, tone='positive', device='cuda', max_length=512):\n","#     model.eval()\n","#     # Incorporate the tone into the input prompt in a way the model understands\n","#     prompt = f\"{input_sentence} [Tone: {tone}]\"  # This is highly model-dependent\n","\n","#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n","#     attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n","\n","#     # Generate paraphrase\n","#     output_sequences = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length)\n","\n","#     # Decode generated sequence to text\n","#     paraphrase = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n","#     return paraphrase"],"metadata":{"id":"Ap8IBFylIQQy","executionInfo":{"status":"ok","timestamp":1708362065309,"user_tz":-60,"elapsed":31,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_dataset = ParaphraseTripletDataset(train_df, tokenizer)\n","print(train_df['original'][:2])\n","print(train_dataset[0]['input_ids'])\n","val_dataset = ParaphraseTripletDataset(val_df, tokenizer)\n","print(val_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TruAh1C-ByKS","executionInfo":{"status":"ok","timestamp":1708363167080,"user_tz":-60,"elapsed":1147,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"c155518b-4ccb-459b-bf49-be6bf66b0c75"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["122    The hotel room is priced competitively.\n","190         The meal's flavors are pronounced.\n","Name: original, dtype: object\n","tensor([ 3856, 27111,    10,   784, 16034,   908,    37,  1595,   562,    19,\n","        10565,  3265,   120,     5,     1,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0])\n","<__main__.ParaphraseTripletDataset object at 0x7b0ab3e6b670>\n"]}]},{"cell_type":"code","source":["train_model(train_dataset, model, tokenizer, device, epochs=3)\n","eval_model(val_dataset, model, tokenizer, device)"],"metadata":{"id":"iS1QvR7AXC71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate a paraphrase\n","sentiment = 'POS'  # or 'NEG'\n","input_sentence = \"The meal was served promptly.\"\n","paraphrase = generate_paraphrase(input_sentence, sentiment, model, tokenizer, device)\n","print(f\"paraphrase: {paraphrase}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdUl6DPMHp-S","executionInfo":{"status":"ok","timestamp":1708362161487,"user_tz":-60,"elapsed":4086,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"6b3c9fe2-b3c1-4dcf-c632-b8c323679b6f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["paraphrase: \n"]}]},{"cell_type":"code","source":["test_df = df.head(20)\n","for index, row in test_df.iterrows():\n","    input_token = tokenizer.encode(\"paraphrase:\" + row['original'], return_tensors=\"pt\").to(device)\n","    generated_ids = model.generate(input_token, max_length=50, num_beams=5, early_stopping=True)\n","    print(\"original: \", row['original'])\n","    print(\"paraphrase: \", tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYk5f3ZARTCK","executionInfo":{"status":"ok","timestamp":1708362165766,"user_tz":-60,"elapsed":4281,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"bae8b094-a6e5-42f9-ceaf-d87a292f36f8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["original:  I usually wake up early in the morning.\n","paraphrase:  \n","\n","original:  I regularly take a walk in the evening.\n","paraphrase:  \n","\n","original:  The weather is unpredictable today.\n","paraphrase:  \n","\n","original:  The restaurant offers food.\n","paraphrase:  \n","\n","original:  This movie features a standard plot.\n","paraphrase:  \n","\n","original:  Our team has bearable performance base.\n","paraphrase:  \n","\n","original:  The new software update includes requiered features.\n","paraphrase:  \n","\n","original:  He completes tasks on time.\n","paraphrase:  \n","\n","original:  The hotel room was immaculately maintained and spotless.\n","paraphrase:  True\n","\n","original:  The book is offered to readers with no review.\n","paraphrase:  \n","\n","original:  Their customer service is mediocre.\n","paraphrase:  \n","\n","original:  She has a common knowledge on the topic.\n","paraphrase:  \n","\n","original:  The garden's normally maintained.\n","paraphrase:  \n","\n","original:  The performance was generally ordinary.\n","paraphrase:  \n","\n","original:  Their response was quick.\n","paraphrase:  \n","\n","original:  The instructions were clear.\n","paraphrase:  \n","\n","original:  The product has given price.\n","paraphrase:  \n","\n","original:  The room provides sufficient space.\n","paraphrase:  \n","\n","original:  The computer operates with common speed.\n","paraphrase:  \n","\n","original:  The meeting's outcomes were balanced.\n","paraphrase:  \n","\n"]}]}]}