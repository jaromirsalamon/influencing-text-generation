{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdueE692dxmBDOxVPctv0u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UuOArv5gmnOA","executionInfo":{"status":"ok","timestamp":1708250176597,"user_tz":-60,"elapsed":21484,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"3c93980f-ccb9-44e6-d55a-05276a839a81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["dataset = 'manual' # 'mrpc','manual'\n","sentiment = 'positive' # 'positive', 'negative'\n","\n","folder_input_path = '/content/drive/My Drive/Colab Notebooks/5_Corpora/corpora/'\n","folder_pretrained_path = '/content/drive/My Drive/Colab Notebooks/8_Text_Paraphrasing/pretrained/'\n","csv_file_path = f'{dataset}-triplet-corpus.csv'"],"metadata":{"id":"BaSMg2xqnALH","executionInfo":{"status":"ok","timestamp":1708250211239,"user_tz":-60,"elapsed":299,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Load and Preprocess the Data"],"metadata":{"id":"XUp6pGS8pgrW"}},{"cell_type":"code","source":["import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","# Load the dataset\n","df = pd.read_csv(folder_input_path + csv_file_path)\n","\n","# Simple preprocessing\n","# Assuming the file has columns 'original' and given by sentiment variable for paraphrase pairs\n","original_sentences = df['original'].tolist()\n","positive_sentences = df[sentiment].tolist()\n","\n","# Combine for vocabulary building\n","all_sentences = original_sentences + positive_sentences"],"metadata":{"id":"CZNsFvsto-ae","executionInfo":{"status":"ok","timestamp":1708250214045,"user_tz":-60,"elapsed":1097,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Build Vocabulary"],"metadata":{"id":"l1up5hqWpiPt"}},{"cell_type":"code","source":["def build_vocab(sentences, min_freq=1):\n","    # Tokenize sentences\n","    tokenized_sentences = [sentence.split() for sentence in sentences]\n","\n","    # Count word frequencies\n","    word_freq = Counter(word for sentence in tokenized_sentences for word in sentence)\n","\n","    # Filter words by min frequency and sort\n","    vocab = [word for word, freq in word_freq.items() if freq >= min_freq]\n","    vocab = sorted(vocab)\n","\n","    # Add <pad> and <unk> tokens\n","    vocab = ['<pad>', '<unk>'] + vocab\n","\n","    # Create word to index mapping\n","    word2idx = {word: idx for idx, word in enumerate(vocab)}\n","\n","    return vocab, word2idx\n","\n","vocab, word2idx = build_vocab(all_sentences)\n","vocab_size = len(vocab)\n","vocab_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnuGiLcqpiue","executionInfo":{"status":"ok","timestamp":1708251589377,"user_tz":-60,"elapsed":323,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"d38fa766-54b5-441d-8b4b-2f9d42b9b095"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1040"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Create Dataset and DataLoader"],"metadata":{"id":"epmlUsnFpzzT"}},{"cell_type":"code","source":["class ParaphraseDataset(Dataset):\n","    def __init__(self, original_sentences, positive_sentences, word2idx):\n","        self.original_sentences = [self.encode(sentence, word2idx) for sentence in original_sentences]\n","        self.positive_sentences = [self.encode(sentence, word2idx) for sentence in positive_sentences]\n","\n","    def encode(self, sentence, word2idx):\n","        return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n","\n","    def pad_sequence(self, sequence):\n","        max_len = max(len(seq) for seq in self.original_sentences + self.positive_sentences)\n","        return [seq + [word2idx['<pad>']] * (max_len - len(seq)) for seq in sequence]\n","\n","    def __getitem__(self, index):\n","        return torch.tensor(self.pad_sequence([self.original_sentences[index]])[0]), torch.tensor(self.pad_sequence([self.positive_sentences[index]])[0])\n","\n","    def __len__(self):\n","        return len(self.original_sentences)\n","\n","dataset = ParaphraseDataset(original_sentences, positive_sentences, word2idx)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"h65J9xKcp0SW","executionInfo":{"status":"ok","timestamp":1708251643147,"user_tz":-60,"elapsed":315,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, input_dim, output_dim, emb_dim, hid_dim, n_layers, dropout=0.1):\n","        super().__init__()\n","        self.encoder = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n","        self.decoder = nn.Linear(hid_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        embedded = self.dropout(self.encoder(src))\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        predictions = self.decoder(outputs)\n","        return predictions\n","\n","# Example usage\n","input_dim = vocab_size  # Size of the input vocabulary\n","output_dim = vocab_size # Size of the output vocabulary\n","emb_dim = 256      # Size of the embeddings\n","hid_dim = 512      # Size of the hidden layers\n","n_layers = 2       # Number of layers in the RNN\n","\n","model = Seq2Seq(input_dim, output_dim, emb_dim, hid_dim, n_layers)"],"metadata":{"id":"-R53HmP_q941","executionInfo":{"status":"ok","timestamp":1708251927285,"user_tz":-60,"elapsed":410,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"_H9_b7btqUmu"}},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])\n","\n","num_epochs = 50\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for original, positive in dataloader:\n","        optimizer.zero_grad()\n","        output = model(original)  # Forward pass, assume output is [batch_size, seq_len, output_dim]\n","        output_dim = output.shape[-1]\n","\n","        # Reshape output to [batch_size, num_classes, seq_len] for CrossEntropyLoss\n","        # output = output.permute(1, 2, 0)  # Now [batch_size, num_classes, seq_len]\n","\n","        output = output.reshape(-1, output_dim) # [batch_size * seq_len, output_dim]\n","        positive = positive.view(-1) # [batch_size * seq_len]\n","\n","        loss = criterion(output, positive) # Compute loss\n","        loss.backward()  # Backpropagation\n","        optimizer.step()  # Update weights\n","        total_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRnm8JXjqWtM","executionInfo":{"status":"ok","timestamp":1708252278224,"user_tz":-60,"elapsed":50932,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"afb5dd68-ef16-4c8b-b65b-c4e8bcd766b2"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.42290156015328\n","Epoch 2, Loss: 0.3658991924354008\n","Epoch 3, Loss: 0.2777726650238037\n","Epoch 4, Loss: 0.23327209055423737\n","Epoch 5, Loss: 0.20547428088528769\n","Epoch 6, Loss: 0.1854569081749235\n","Epoch 7, Loss: 0.1674469666821616\n","Epoch 8, Loss: 0.1586049965449742\n","Epoch 9, Loss: 0.14113516360521317\n","Epoch 10, Loss: 0.13711594258035933\n","Epoch 11, Loss: 0.12984411524874823\n","Epoch 12, Loss: 0.1260456389614514\n","Epoch 13, Loss: 0.12606431863137654\n","Epoch 14, Loss: 0.1192490713936942\n","Epoch 15, Loss: 0.11300882803542274\n","Epoch 16, Loss: 0.11184119752475194\n","Epoch 17, Loss: 0.11227015405893326\n","Epoch 18, Loss: 0.11348794187818255\n","Epoch 19, Loss: 0.1080552414059639\n","Epoch 20, Loss: 0.11902696426425662\n","Epoch 21, Loss: 0.12472308001347951\n","Epoch 22, Loss: 0.12079328830753054\n","Epoch 23, Loss: 0.10941466582672936\n","Epoch 24, Loss: 0.1070938626570361\n","Epoch 25, Loss: 0.1043108164199761\n","Epoch 26, Loss: 0.09549398081643241\n","Epoch 27, Loss: 0.09559263927595955\n","Epoch 28, Loss: 0.09538963969264712\n","Epoch 29, Loss: 0.09424726984330586\n","Epoch 30, Loss: 0.09540861525705882\n","Epoch 31, Loss: 0.09271708237273353\n","Epoch 32, Loss: 0.09295538227472987\n","Epoch 33, Loss: 0.09063452216131347\n","Epoch 34, Loss: 0.09100719115563802\n","Epoch 35, Loss: 0.0900981170790536\n","Epoch 36, Loss: 0.08972626179456711\n","Epoch 37, Loss: 0.09304164775780269\n","Epoch 38, Loss: 0.09482034295797348\n","Epoch 39, Loss: 0.08900928124785423\n","Epoch 40, Loss: 0.0900700603212629\n","Epoch 41, Loss: 0.0884889428104673\n","Epoch 42, Loss: 0.08743755146861076\n","Epoch 43, Loss: 0.0846934834761279\n","Epoch 44, Loss: 0.08967820980719157\n","Epoch 45, Loss: 0.08359797458563532\n","Epoch 46, Loss: 0.08746183876480375\n","Epoch 47, Loss: 0.0895450312112059\n","Epoch 48, Loss: 0.09008808167917388\n","Epoch 49, Loss: 0.08692456994737897\n","Epoch 50, Loss: 0.08514523612601417\n"]}]},{"cell_type":"code","source":["def generate_paraphrase(sentence, word2idx, idx2word, model, max_length=50):\n","    model.eval()\n","    tokens = [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n","    tokens_tensor = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n","    with torch.no_grad():\n","        output = model(tokens_tensor)\n","    output_indices = output.argmax(2)  # Get the index with the highest probability\n","    paraphrased_sentence = ' '.join(idx2word[idx.item()] for idx in output_indices[0] if idx != word2idx['<pad>'])  # Decode\n","    return paraphrased_sentence\n","\n","# Example usage\n","sentence = \"This is an example sentence to paraphrase.\"\n","idx2word = {idx: word for word, idx in word2idx.items()}  # Create inverse mapping\n","paraphrased_sentence = generate_paraphrase(sentence, word2idx, idx2word, model)\n","print(paraphrased_sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWaQaSGavpos","executionInfo":{"status":"ok","timestamp":1708252690495,"user_tz":-60,"elapsed":342,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"47a580ff-7e24-410d-f020-6365061d111b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a a a level of\n"]}]}]}