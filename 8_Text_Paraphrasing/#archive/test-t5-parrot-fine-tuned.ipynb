{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMG82+TSyBy814V2ylq5Gii"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LEJrLxihGerq","executionInfo":{"status":"ok","timestamp":1708292754097,"user_tz":-60,"elapsed":21542,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"e63c0654-4bb7-4ad0-9171-f3c13dd0bebf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","folder_pretrained_path = '/content/drive/My Drive/Colab Notebooks/8_Text_Paraphrasing/pretrained'\n","checkpoint_path = os.path.join(folder_pretrained_path, 'checkpoint-159')"],"metadata":{"id":"QQlOhH-iGpqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mbf925qdBM3O","executionInfo":{"status":"ok","timestamp":1708292852623,"user_tz":-60,"elapsed":38398,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"b8d3bfd8-c1d8-4c59-b096-af621b63c564"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Load the model and tokenizer\n","model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n","tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)"]},{"cell_type":"code","source":["input_text = \"paraphrase: This is the sentence I want to paraphrase.\""],"metadata":{"id":"yz8VbgzALAir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","# Encode the input text\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","\n","# If you're using a GPU, move the tensors to GPU (assuming CUDA is available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","input_ids = input_ids.to(device)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XCNk0OlLGCB","executionInfo":{"status":"ok","timestamp":1708293920427,"user_tz":-60,"elapsed":518,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"15d4cbd8-9eae-4c39-fada-48eb7d625c25"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-11): 11 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Generate outputs\n","outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f'text:{generated_text}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT0Ev9gEMfee","executionInfo":{"status":"ok","timestamp":1708293926824,"user_tz":-60,"elapsed":1384,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"b9e58fcd-9ae7-4a03-eaf9-ea8b4d09173d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["text:\n"]}]},{"cell_type":"code","source":["model.eval()  # Put the model in evaluation mode\n","\n","input_text = \"This is the sentence I want to paraphrase.\"  # Example input text\n","input_text = \"paraphrase: \" + input_text  # Prefixing for T5 models\n","\n","# Encode the input text\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","input_ids = input_ids.to(device)\n","\n","# Generate outputs\n","outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f\"Generated text: {generated_text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lz1GZLWuQmfy","executionInfo":{"status":"ok","timestamp":1708294102346,"user_tz":-60,"elapsed":1294,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"5823804f-3dee-4c8d-9835-da07915bb531"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated text: \n"]}]},{"cell_type":"code","source":["# Generate outputs\n","outputs = model.generate(input_ids)\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f\"Generated text: {generated_text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoS7f84NQ-2s","executionInfo":{"status":"ok","timestamp":1708294154436,"user_tz":-60,"elapsed":2600,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"2d398b3c-4b62-4edd-c61d-471f549f5fd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Generated text: \n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Make sure to load the model and tokenizer here\n","model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n","tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","# Choose the device (GPU or CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Prepare the text input for paraphrasing\n","input_text = \"The meal was served promptly.\"\n","# input_text = \"paraphrase: \" + input_text  # Prefixing for T5 models\n","\n","# Encode the input text\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n","\n","# Generate outputs\n","# outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n","outputs = model.generate(input_ids, max_length=50)\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f\"Generated text: {generated_text}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64nERQGiR9mp","executionInfo":{"status":"ok","timestamp":1708294620837,"user_tz":-60,"elapsed":7421,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"e5882139-7f8a-422a-d040-4062f51f16f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["Generated text: \n"]}]},{"cell_type":"code","source":["import transformers\n","transformers.logging.set_verbosity_info()\n","\n","model.eval()  # Put the model in evaluation mode\n","\n","# Ensure the model is on the correct device\n","model.to(device)\n","\n","input_text = \"This is the sentence I want to paraphrase.\"\n","input_text = \"paraphrase: \" + input_text\n","\n","# Encode the input text and print out the input IDs\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n","print(f\"Input IDs: {input_ids}\")\n","\n","# Generate outputs without early stopping\n","# outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=False)\n","outputs = model.generate(input_ids, max_length=50)\n","\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f\"Generated text: {generated_text}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbXgFNFVUxpD","executionInfo":{"status":"ok","timestamp":1708295242164,"user_tz":-60,"elapsed":1536,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"36540926-b3d3-4adf-debb-4b0fce5d269a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Input IDs: tensor([[ 3856, 27111,    10,   100,    19,     8,  7142,    27,   241,    12,\n","          3856, 27111,     5,     1]], device='cuda:0')\n","Generated text: \n"]}]},{"cell_type":"code","source":["# Try with very simple input\n","input_text = \"paraphrase: The quick brown fox jumps over the lazy dog.\"\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n","\n","# Simplify the generation call to the most basic form\n","outputs = model.generate(input_ids)\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f\"Generated text: {generated_text}\")\n"],"metadata":{"id":"8Cw376XjVo4j","executionInfo":{"status":"ok","timestamp":1708295367680,"user_tz":-60,"elapsed":1338,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"2c8ef96f-1237-421a-b909-7222aa2316ad","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Generated text: \n"]}]},{"cell_type":"code","source":["# Check for all zero or NaN values in the model weights\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(name, torch.any(param.data != 0), torch.any(torch.isnan(param.data)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJcOr_wmV6rC","executionInfo":{"status":"ok","timestamp":1708295437707,"user_tz":-60,"elapsed":578,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"f00e6722-5542-4dc3-a942-592fcce4fe83"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["shared.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.0.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.1.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.2.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.3.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.4.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.5.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.6.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.7.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.8.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.9.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.10.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.1.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.1.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.block.11.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","encoder.final_layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.0.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.1.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.2.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.3.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.4.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.5.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.6.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.7.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.8.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.9.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.10.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.0.SelfAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.0.SelfAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.0.SelfAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.0.SelfAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.0.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.1.EncDecAttention.q.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.1.EncDecAttention.k.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.1.EncDecAttention.v.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.1.EncDecAttention.o.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.1.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.2.DenseReluDense.wi.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.2.DenseReluDense.wo.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.block.11.layer.2.layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n","decoder.final_layer_norm.weight tensor(True, device='cuda:0') tensor(False, device='cuda:0')\n"]}]},{"cell_type":"code","source":["# Use default settings for generation\n","outputs = model.generate(input_ids)\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f\"Generated text: {generated_text}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YQJxBjKWJAH","executionInfo":{"status":"ok","timestamp":1708295495366,"user_tz":-60,"elapsed":770,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"bd09093a-cb1a-4516-e542-05fa48f603ca"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Generated text: \n"]}]},{"cell_type":"code","source":["torch.autograd.set_detect_anomaly(True)\n","outputs = model.generate(input_ids)\n","\n","# Decode the generated output\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(f\"Generated text: {generated_text}\")"],"metadata":{"id":"phS7k3AaWimr","executionInfo":{"status":"ok","timestamp":1708295614393,"user_tz":-60,"elapsed":776,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"7da6ef73-4147-478f-f751-b52346e49675","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["Generated text: \n"]}]}]}