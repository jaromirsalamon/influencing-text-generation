{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO12kBDjsuHw7hSN4dqzEOj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Based on https://medium.com/p/229ca998d229"],"metadata":{"id":"OfF-SBU8u2jd"}},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_Spac0LvRrT","executionInfo":{"status":"ok","timestamp":1702228751282,"user_tz":-60,"elapsed":10338,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"dcac1276-d62d-4d2d-e4d7-1b722b9e6406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R38CRCz6udnF"},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","model = T5ForConditionalGeneration.from_pretrained('prithivida/parrot_paraphraser_on_T5')\n","tokenizer = T5Tokenizer.from_pretrained('prithivida/parrot_paraphraser_on_T5', legacy=False)"]},{"cell_type":"markdown","source":["UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","\n","UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation."],"metadata":{"id":"UzPwiZAyyY5K"}},{"cell_type":"code","source":["batch = tokenizer(\"Natural Language Processing can improve the quality life.\", return_tensors='pt')\n","\n","generated_ids = model.generate( batch['input_ids'],\n","                                num_beams=5,\n","                                #num_return_sequences\n","                                temperature=1.5,\n","                                #num_beam_groups\n","                                #diversity_penalty\n","                                no_repeat_ngram_size=2,\n","                                early_stopping=True,\n","                                length_penalty=2.0)\n","\n","generated_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","print( generated_sentence )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWVEbeFlx_Er","executionInfo":{"status":"ok","timestamp":1702229457362,"user_tz":-60,"elapsed":4176,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"869aece5-de7e-4d95-f882-77aad5261010"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["['Natural language processing can improve the quality of life.']\n"]}]},{"cell_type":"code","source":["batch = tokenizer(\"Natural Language Processing can improve the quality life.\", return_tensors='pt')\n","\n","generated_ids = model.generate( batch['input_ids'],\n","                                num_beams=5,\n","                                num_return_sequences=5,\n","                                temperature=1.5,\n","                                num_beam_groups=5,\n","                                diversity_penalty=2.0,\n","                                no_repeat_ngram_size=2,\n","                                early_stopping=True,\n","                                length_penalty=2.0)\n","\n","generated_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","print( generated_sentence )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CObRQ3WT-EhH","executionInfo":{"status":"ok","timestamp":1702232628370,"user_tz":-60,"elapsed":4793,"user":{"displayName":"Jaromír Salamon","userId":"07277902595109530723"}},"outputId":"72ad096d-c88d-4029-92a5-afb04daf91cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["['Natural language processing can improve the quality of life.', 'Natural Language Processing is a tool that can improve the quality of life.', 'Natural Language Processing can improve quality of life.', 'Nature can improve the quality of life.', 'Natural language processing improves life.']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HC403xNo-Bf6"},"execution_count":null,"outputs":[]}]}